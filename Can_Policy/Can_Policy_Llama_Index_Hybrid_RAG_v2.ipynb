{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b3dee632a91a4ab78d91e9b99acd64f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cd2eabdb4bde4faf85afaa46fa62ce61",
              "IPY_MODEL_9d4a1b6dd5bf4b6487c47aa7b845ef42",
              "IPY_MODEL_3b30d66015674af6bb46591668967e7c"
            ],
            "layout": "IPY_MODEL_cc3558daac8148f09c4a1b0ecc48faa7"
          }
        },
        "cd2eabdb4bde4faf85afaa46fa62ce61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b470d75887a488a8fa4177fd389b8ca",
            "placeholder": "​",
            "style": "IPY_MODEL_c6b549069fa841d6bc4d96c226b77c08",
            "value": "Fetching 5 files: 100%"
          }
        },
        "9d4a1b6dd5bf4b6487c47aa7b845ef42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b6c6c30eab04018ab3d5d9e01819f4a",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c6103cdeea244254b9b568e5ff4881d9",
            "value": 5
          }
        },
        "3b30d66015674af6bb46591668967e7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7cefd09e370f43809da0d41080220144",
            "placeholder": "​",
            "style": "IPY_MODEL_2bcd69e8ecbd44e1bcc1aedd7236a5df",
            "value": " 5/5 [00:00&lt;00:00, 149.52it/s]"
          }
        },
        "cc3558daac8148f09c4a1b0ecc48faa7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b470d75887a488a8fa4177fd389b8ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6b549069fa841d6bc4d96c226b77c08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0b6c6c30eab04018ab3d5d9e01819f4a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6103cdeea244254b9b568e5ff4881d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7cefd09e370f43809da0d41080220144": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2bcd69e8ecbd44e1bcc1aedd7236a5df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index --quiet\n",
        "!pip install neo4j --quiet\n",
        "!pip install llama-index-graph-stores-neo4j --quiet\n",
        "!pip install llama-parse --quiet\n",
        "!pip install qdrant_client --quiet\n",
        "!pip install llama-index-vector-stores-qdrant --quiet\n",
        "!pip install llama-index-embeddings-huggingface --quiet\n",
        "!pip install llama-index-embeddings-fastembed --quiet\n",
        "!pip install llama-index-llms-groq --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kgn2mh7E2yoL",
        "outputId": "169aa1a5-2b5d-43e3-a859-0909d6b18163",
        "collapsed": true
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.2/180.2 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.7/365.7 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.4/259.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.6/316.6 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires protobuf<5,>=3.20, but you have protobuf 5.28.0 which is incompatible.\n",
            "google-ai-generativelanguage 0.6.6 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.28.0 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.25.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.28.0 which is incompatible.\n",
            "google-cloud-datastore 2.19.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.28.0 which is incompatible.\n",
            "google-cloud-firestore 2.16.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.28.0 which is incompatible.\n",
            "tensorboard 2.17.0 requires protobuf!=4.24.0,<5.0.0,>=3.19.6, but you have protobuf 5.28.0 which is incompatible.\n",
            "tensorflow 2.17.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.28.0 which is incompatible.\n",
            "tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 5.28.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m861.9/861.9 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.0/303.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.6/55.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for PyStemmer (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "from typing import List, Dict, Any\n",
        "import json\n",
        "import re\n",
        "from llama_index.core import VectorStoreIndex, StorageContext\n",
        "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
        "from llama_index.graph_stores.neo4j import Neo4jPropertyGraphStore\n",
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "from llama_index.core.retrievers import VectorIndexRetriever\n",
        "from llama_index.core.node_parser import SimpleNodeParser\n",
        "from llama_index.core.indices.keyword_table import KeywordTableIndex\n",
        "from llama_index.embeddings.fastembed import FastEmbedEmbedding\n",
        "from llama_index.llms.groq import Groq\n",
        "from llama_index.core import Settings, PropertyGraphIndex\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from llama_index.core import PromptTemplate\n",
        "from llama_index.core.tools import QueryEngineTool\n",
        "from llama_index.core.query_engine import RouterQueryEngine\n",
        "from llama_index.core.selectors import LLMSingleSelector, LLMMultiSelector\n",
        "from llama_index.core.selectors import (\n",
        "    PydanticMultiSelector,\n",
        "    PydanticSingleSelector,\n",
        ")\n",
        "from llama_index.core.response_synthesizers import TreeSummarize\n",
        "\n",
        "import numpy as np\n",
        "from google.colab import userdata\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "class IntegratedKnowledgeBaseQuery:\n",
        "    def __init__(self):\n",
        "        self.embed_model, self.llm = self._initialize_components()\n",
        "        self.graph_store = self._setup_graph_store()\n",
        "        self.vector_store = self._setup_vector_store()\n",
        "        self.graph_index, self.vector_index = self._setup_index()\n",
        "\n",
        "    def _initialize_components(self):\n",
        "        embed_model = FastEmbedEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "        Settings.embed_model = embed_model\n",
        "        llm = Groq(model=\"llama3-70b-8192\", api_key=userdata.get('GROQ_API_KEY'), temperature=0)\n",
        "        Settings.llm = llm\n",
        "\n",
        "        return embed_model, llm\n",
        "\n",
        "    def _setup_graph_store(self):\n",
        "        return Neo4jPropertyGraphStore(\n",
        "            username='neo4j',\n",
        "            password=userdata.get('NEO4J_PASSWORD'),\n",
        "            url=userdata.get('NEO4J_URL'),\n",
        "            database=\"neo4j\",  # Specify the database name if needed\n",
        "            refresh_schema=False,  # Disable automatic schema refresh\n",
        "            sanitize_query_output=True  # Enable query output sanitization\n",
        "        )\n",
        "\n",
        "    def get_neo4j_schema(self):\n",
        "        cypher_query = \"\"\"\n",
        "        CALL db.schema.visualization()\n",
        "        \"\"\"\n",
        "        try:\n",
        "            result = self.graph_store.structured_query(cypher_query)\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error retrieving Neo4j schema: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def _setup_vector_store(self):\n",
        "        return QdrantVectorStore(\n",
        "            url=userdata.get('QDRANT_URL'),\n",
        "            api_key=userdata.get('QDRANT_API_KEY'),\n",
        "            collection_name=\"legislative_docs\",\n",
        "        )\n",
        "\n",
        "    def _setup_index(self):\n",
        "        storage_context = StorageContext.from_defaults(\n",
        "            vector_store=self.vector_store,\n",
        "            graph_store=self.graph_store,\n",
        "        )\n",
        "        graph_index = PropertyGraphIndex.from_existing(\n",
        "            property_graph_store=self.graph_store,\n",
        "            storage_context=storage_context)\n",
        "\n",
        "        vector_index = VectorStoreIndex.from_vector_store(\n",
        "            vector_store=self.vector_store,\n",
        "            storage_context=storage_context,\n",
        "        )\n",
        "        return graph_index, vector_index\n",
        "\n",
        "    def diagnose_stores(self):\n",
        "        logging.info(\"Diagnosing graph store...\")\n",
        "        self._diagnose_graph_store()\n",
        "        logging.info(\"Diagnosing vector store...\")\n",
        "        self._diagnose_vector_store()\n",
        "\n",
        "    def _diagnose_graph_store(self):\n",
        "        query = \"MATCH (n) RETURN count(n) as node_count\"\n",
        "        result = self.graph_store.structured_query(query)\n",
        "        node_count = result[0]['node_count']\n",
        "        logging.info(f\"Total nodes in the graph: {node_count}\")\n",
        "\n",
        "        query = \"MATCH (n) RETURN DISTINCT labels(n) as node_types\"\n",
        "        result = self.graph_store.structured_query(query)\n",
        "        node_types = [r['node_types'][0] for r in result if r['node_types']]\n",
        "        logging.info(f\"Node types in the graph: {', '.join(node_types)}\")\n",
        "\n",
        "        query = \"MATCH (n) RETURN n LIMIT 5\"\n",
        "        result = self.graph_store.structured_query(query)\n",
        "        logging.info(\"Sample nodes:\")\n",
        "        for record in result:\n",
        "            logging.info(record['n'])\n",
        "\n",
        "    def _diagnose_vector_store(self):\n",
        "        collection_info = self.vector_store.client.get_collection(collection_name=\"legislative_docs\")\n",
        "        logging.info(f\"Vector store collection info: {collection_info}\")\n",
        "\n",
        "    def query_graph_store(self, query: str) -> List[Dict[str, Any]]:\n",
        "        entities = re.findall(r'\\b(?:C-\\d+|Bill C-\\d+|[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)\\b', query, re.IGNORECASE)\n",
        "        logging.info(f\"Entities found: {entities}\")\n",
        "        cypher_query = \"\"\"\n",
        "        MATCH (e)\n",
        "        WHERE toLower(e.name) CONTAINS toLower($entity_name) OR toLower(e.number) CONTAINS toLower($entity_name)\n",
        "        OPTIONAL MATCH (e)-[r]-(related)\n",
        "        RETURN e as entity, type(r) as relationship_type, related\n",
        "        LIMIT 5\n",
        "        \"\"\"\n",
        "\n",
        "        graph_results = []\n",
        "        for entity in entities:\n",
        "            params = {\"entity_name\": entity}\n",
        "            try:\n",
        "                results = self.graph_store.structured_query(cypher_query, params)\n",
        "                graph_results.extend(results)\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error querying graph store for entity '{entity}': {str(e)}\")\n",
        "\n",
        "        return graph_results\n",
        "\n",
        "    def get_bill_details(self, bill_id):\n",
        "        cypher_query = \"\"\"\n",
        "        MATCH (b:Bill {id: $bill_id})\n",
        "        OPTIONAL MATCH (b)-[:AMENDS]->(a:Amendment)\n",
        "        OPTIONAL MATCH (b)-[:CONTAINS]->(p:Provision)\n",
        "        OPTIONAL MATCH (b)-[:DEFINES]->(d:Definition)\n",
        "        OPTIONAL MATCH (b)-[:INVOLVED]->(person:Person)\n",
        "        OPTIONAL MATCH (b)-[:RELATES_TO]->(act:Act)\n",
        "        OPTIONAL MATCH (b)-[:AFFECTS]->(affected)\n",
        "        RETURN b as bill,\n",
        "              collect(DISTINCT a) as amendments,\n",
        "              collect(DISTINCT p) as provisions,\n",
        "              collect(DISTINCT d) as definitions,\n",
        "              collect(DISTINCT person) as persons_involved,\n",
        "              collect(DISTINCT act) as related_acts,\n",
        "              collect(DISTINCT {type: labels(affected)[0], details: affected}) as affected_entities\n",
        "        \"\"\"\n",
        "        results = self.graph_store.structured_query(cypher_query, {\"bill_id\": bill_id})\n",
        "        return results\n",
        "\n",
        "    def format_bill_details(self, results):\n",
        "        if not results:\n",
        "            return \"No bill found with the given ID.\"\n",
        "\n",
        "        bill = results[0]['bill']\n",
        "        formatted_result = f\"Bill: {bill.get('number', 'Unknown')}\\n\"\n",
        "        formatted_result += f\"Title: {bill.get('title', 'Unknown')}\\n\"\n",
        "        formatted_result += f\"Stage: {bill.get('stage', 'Unknown')}\\n\"\n",
        "        formatted_result += f\"Assent Date: {bill.get('assent_date', 'Unknown')}\\n\\n\"\n",
        "\n",
        "        formatted_result += \"Amendments:\\n\" + \"\\n\".join([f\"- {a.get('act_name', 'Unknown')}: {a.get('description', 'No description')}\" for a in results[0]['amendments']]) + \"\\n\\n\"\n",
        "        formatted_result += \"Key Provisions:\\n\" + \"\\n\".join([f\"- {p.get('title', 'Unknown')}: {p.get('description', 'No description')}\" for p in results[0]['provisions']]) + \"\\n\\n\"\n",
        "        formatted_result += \"Definitions:\\n\" + \"\\n\".join([f\"- {d.get('term', 'Unknown')}: {d.get('definition', 'No definition')}\" for d in results[0]['definitions']]) + \"\\n\\n\"\n",
        "        formatted_result += \"Persons Involved:\\n\" + \"\\n\".join([f\"- {p.get('name', 'Unknown')} ({p.get('position', 'Unknown position')})\" for p in results[0]['persons_involved']]) + \"\\n\\n\"\n",
        "        formatted_result += \"Related Acts:\\n\" + \"\\n\".join([f\"- {a.get('name', 'Unknown')}\" for a in results[0]['related_acts']]) + \"\\n\\n\"\n",
        "        formatted_result += \"Affected Entities:\\n\" + \"\\n\".join([f\"- {e['type']}: {e['details'].get('name', 'Unknown')}\" for e in results[0]['affected_entities']]) + \"\\n\"\n",
        "\n",
        "        return formatted_result\n",
        "\n",
        "    def format_graph_results(self, query):\n",
        "        graph_results = self.query_graph_store(query)\n",
        "        logging.info (f\"format_graph_results function: {graph_results}\")\n",
        "        if not graph_results:\n",
        "            return \"No relevant information found in the graph database.\", []\n",
        "\n",
        "        formatted_results = []\n",
        "        bill_details = []\n",
        "        for result in graph_results:\n",
        "            bill = result['entity']\n",
        "            relationship = result['relationship_type']\n",
        "            related = result['related']\n",
        "\n",
        "            formatted_result = f\"Bill {bill.get('number', 'Unknown')}: {bill.get('title', 'Unknown')}\"\n",
        "            if relationship and related:\n",
        "                related_name = related.get('name', related.get('number', 'Unknown'))\n",
        "                formatted_result += f\"\\n  {relationship}: {related_name}\"\n",
        "\n",
        "            formatted_results.append(formatted_result)\n",
        "\n",
        "            bill_detail = self.get_bill_details(bill.get('id'))\n",
        "            if bill_detail:\n",
        "                formatted_bill = self.format_bill_details(bill_detail)\n",
        "                bill_details.append(formatted_bill)\n",
        "\n",
        "        return \"\\n\\n\".join(formatted_results), bill_details\n",
        "\n",
        "    def format_vector_results(self, query) -> str:\n",
        "        query_vector = self.embed_model.get_text_embedding(query)\n",
        "        vector_results = self.vector_store.client.search(\n",
        "            collection_name=\"legislative_docs\",\n",
        "            query_vector=query_vector,\n",
        "            limit=2\n",
        "        )\n",
        "        formatted_results = []\n",
        "        for i, result in enumerate(vector_results, 1):\n",
        "            payload = result.payload\n",
        "            score = result.score\n",
        "            node_content = json.loads(result.payload['_node_content'])\n",
        "            content = node_content.get('text', '')\n",
        "            formatted_results.append(f\"Document {i} (Score: {score:.4f}):\\n{content[:300]}...\")\n",
        "        return \"\\n\".join(formatted_results)\n",
        "\n",
        "    async def generate_llm_response(self, query: str, response: str, graph_results: str, vector_results: str, bill_details: List[str]) -> str:\n",
        "        prompt = f\"\"\"You are a highly knowledgeable Legal AI assistant specializing in analyzing legislative documents and bills. Your task is to provide a comprehensive and accurate response to the following query based on the information from both a graph database and a vector database.\n",
        "\n",
        "        Query: {query}\n",
        "\n",
        "        Initial Response: {response}\n",
        "\n",
        "        Knowledge Context: {graph_results} + {vector_results}\n",
        "\n",
        "        Bill Details: {bill_details}\n",
        "\n",
        "        Instructions:\n",
        "        1. Analyze all provided contexts, extracting all relevant information related to the query.\n",
        "        2. Provide a clear, concise, and well-structured response that directly addresses the query.\n",
        "        3. Include specific details such as bill numbers, dates, amendments, key provisions, and related entities when available in any context.\n",
        "        4. If the contexts contain information about multiple related bills or legal issues, summarize each one briefly and explain their relevance to the query.\n",
        "        5. If there are any conflicting opinions or interpretations in the contexts, present them objectively and explain the implications.\n",
        "        6. Use legal terminology accurately, but also provide explanations for complex terms to ensure clarity.\n",
        "        7. If the contexts don't provide sufficient information to fully answer the query, clearly state what is known and what information is missing.\n",
        "        8. Do not make assumptions or include information not present in the given contexts.\n",
        "        9. Conclude your response with a brief summary of the key points.\n",
        "        10. After your main response, suggest two follow-up questions that would be relevant for further exploration of the topic, prefaced with \"For further exploration, you might consider asking:\".\n",
        "\n",
        "        Remember to maintain an objective, professional tone throughout your response. Do not refer to the query or contexts directly in your answer; instead, incorporate the information seamlessly into your response.\n",
        "\n",
        "        Now, based on these instructions, please provide your comprehensive analysis and response.\"\"\"\n",
        "\n",
        "        llm_output = self.llm.complete(prompt).text\n",
        "        return llm_output\n",
        "\n",
        "    async def query_knowledge_base(self, query: str) -> str:\n",
        "        logging.info(f\"Querying knowledge base: {query}\")\n",
        "        try:\n",
        "            # Create query engines\n",
        "            graph_query_engine = self.graph_index.as_query_engine()\n",
        "            vector_query_engine = self.vector_index.as_query_engine()\n",
        "\n",
        "            # Create tools\n",
        "            graph_tool = QueryEngineTool.from_defaults(\n",
        "                query_engine=graph_query_engine,\n",
        "                description=\"Useful for answering questions about relationships and connections between entities\",\n",
        "            )\n",
        "\n",
        "            vector_tool = QueryEngineTool.from_defaults(\n",
        "                query_engine=vector_query_engine,\n",
        "                description=\"Useful for answering detailed questions about legal content\",\n",
        "            )\n",
        "\n",
        "            TREE_SUMMARIZE_PROMPT_TMPL = (\n",
        "                \"\"\"You are a helpful legal AI assistant specialized in understanding the legislative enquiries\"\"\"\n",
        "            )\n",
        "            tree_summarize = TreeSummarize(\n",
        "                summary_template=PromptTemplate(TREE_SUMMARIZE_PROMPT_TMPL)\n",
        "            )\n",
        "\n",
        "            # Create router query engine\n",
        "            router_query_engine = RouterQueryEngine(\n",
        "                selector=LLMMultiSelector.from_defaults(),\n",
        "                query_engine_tools=[graph_tool, vector_tool],\n",
        "                summarizer=tree_summarize,\n",
        "            )\n",
        "\n",
        "            # Execute query\n",
        "            initial_response = router_query_engine.query(query)\n",
        "\n",
        "            graph_results, bill_details = self.format_graph_results(query)\n",
        "            logging.info(f\"Formatted graph results: {graph_results}\")\n",
        "            logging.info(f\"Bill details: {bill_details}\")\n",
        "\n",
        "            vector_results = self.format_vector_results(query)\n",
        "            logging.info(f\"Formatted vector results: {vector_results}\")\n",
        "\n",
        "            llm_response = await self.generate_llm_response(query, str(initial_response), graph_results, vector_results, bill_details)\n",
        "            logging.info(f\"LLM response: {llm_response}\")\n",
        "\n",
        "            return llm_response\n",
        "        except Exception as e:\n",
        "            logging.error(f\"An error occurred while querying the knowledge base: {str(e)}\")\n",
        "            return f\"I'm sorry, but an error occurred while processing your query. Please try again or rephrase your question. Error details: {str(e)}\"\n",
        "\n",
        "async def main():\n",
        "    kb_query = IntegratedKnowledgeBaseQuery()\n",
        "    kb_query.diagnose_stores()\n",
        "\n",
        "    while True:\n",
        "        print(\"\\n--- Legal AI Knowledge Base ---\")\n",
        "        query = input(\"\\nEnter your query (or 'quit' to exit): \")\n",
        "        if query.lower() == 'quit':\n",
        "            break\n",
        "        response = await kb_query.query_knowledge_base(query)\n",
        "        print(response)\n",
        "        print(\"\\n\" + \"=\" * 100 + \"\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import asyncio\n",
        "    asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 886,
          "referenced_widgets": [
            "b3dee632a91a4ab78d91e9b99acd64f0",
            "cd2eabdb4bde4faf85afaa46fa62ce61",
            "9d4a1b6dd5bf4b6487c47aa7b845ef42",
            "3b30d66015674af6bb46591668967e7c",
            "cc3558daac8148f09c4a1b0ecc48faa7",
            "7b470d75887a488a8fa4177fd389b8ca",
            "c6b549069fa841d6bc4d96c226b77c08",
            "0b6c6c30eab04018ab3d5d9e01819f4a",
            "c6103cdeea244254b9b568e5ff4881d9",
            "7cefd09e370f43809da0d41080220144",
            "2bcd69e8ecbd44e1bcc1aedd7236a5df"
          ]
        },
        "id": "Dt9uDrxGTDc2",
        "outputId": "8fd2ff67-31ab-4c3f-bde0-9ad904045591"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b3dee632a91a4ab78d91e9b99acd64f0"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Legal AI Knowledge Base ---\n",
            "\n",
            "Enter your query (or 'quit' to exit): Tell me about bill c-10\n",
            "Bill C-10, also known as An Act respecting certain measures related to COVID-19, is a legislative document that was assented to on March 4, 2022. The bill authorizes the Minister of Health to make payments of up to $2.5 billion out of the Consolidated Revenue Fund for COVID-19 tests and to transfer COVID-19 tests and instruments to provinces, territories, bodies, and persons in Canada.\n",
            "\n",
            "The key provisions of Bill C-10 include payments out of the Consolidated Revenue Fund and transfers of COVID-19 tests and instruments. The Minister of Health is involved in the implementation of these provisions. The affected entities include government entities, such as provinces and territories, as well as private entities, including bodies and persons in Canada.\n",
            "\n",
            "It is worth noting that Bill C-10 is mentioned in the context of other legislative documents, including Bill C-8 and Bill C-19. Bill C-8 is referenced in a coordinating amendment, while Bill C-19 is mentioned in a separate document that discusses budget implementation and other measures.\n",
            "\n",
            "In summary, Bill C-10 is a legislative document that authorizes the Minister of Health to make payments and transfers related to COVID-19 tests and instruments. The bill affects government and private entities in Canada and is related to other legislative documents, including Bill C-8 and Bill C-19.\n",
            "\n",
            "For further exploration, you might consider asking:\n",
            "\n",
            "* What are the specific guidelines and criteria for the Minister of Health to make payments and transfers under Bill C-10?\n",
            "* How does Bill C-10 interact with other legislative documents, such as Bill C-8 and Bill C-19, to address the COVID-19 pandemic in Canada?\n",
            "\n",
            "====================================================================================================\n",
            "\n",
            "\n",
            "--- Legal AI Knowledge Base ---\n",
            "\n",
            "Enter your query (or 'quit' to exit): How does Bill C-10 interact with other legislative documents, such as Bill C-8 and Bill C-19, to address the COVID-19 pandemic in Canada?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:llama_index.llms.openai.utils:Retrying llama_index.llms.openai.base.OpenAI._chat in 0.7986920727732159 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01j3h4mrfbehht340pn5ra6xwh` on tokens per minute (TPM): Limit 6000, Used 0, Requested 6242. Please try again in 2.42s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}.\n",
            "WARNING:llama_index.llms.openai.utils:Retrying llama_index.llms.openai.base.OpenAI._chat in 0.5811369668883333 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01j3h4mrfbehht340pn5ra6xwh` on tokens per minute (TPM): Limit 6000, Used 0, Requested 6242. Please try again in 2.42s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}.\n",
            "ERROR:root:An error occurred while querying the knowledge base: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01j3h4mrfbehht340pn5ra6xwh` on tokens per minute (TPM): Limit 6000, Used 0, Requested 6242. Please try again in 2.42s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I'm sorry, but an error occurred while processing your query. Please try again or rephrase your question. Error details: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01j3h4mrfbehht340pn5ra6xwh` on tokens per minute (TPM): Limit 6000, Used 0, Requested 6242. Please try again in 2.42s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
            "\n",
            "====================================================================================================\n",
            "\n",
            "\n",
            "--- Legal AI Knowledge Base ---\n",
            "\n",
            "Enter your query (or 'quit' to exit): How does Bill C-10 interact with other legislative documents, such as Bill C-8 and Bill C-19, to address the COVID-19 pandemic in Canada?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:llama_index.llms.openai.utils:Retrying llama_index.llms.openai.base.OpenAI._chat in 0.9619846535687736 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01j3h4mrfbehht340pn5ra6xwh` on tokens per minute (TPM): Limit 6000, Used 0, Requested 6242. Please try again in 2.42s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}.\n",
            "WARNING:llama_index.llms.openai.utils:Retrying llama_index.llms.openai.base.OpenAI._chat in 0.2551729135089962 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01j3h4mrfbehht340pn5ra6xwh` on tokens per minute (TPM): Limit 6000, Used 0, Requested 6242. Please try again in 2.42s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}.\n",
            "ERROR:root:An error occurred while querying the knowledge base: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01j3h4mrfbehht340pn5ra6xwh` on tokens per minute (TPM): Limit 6000, Used 0, Requested 6242. Please try again in 2.42s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I'm sorry, but an error occurred while processing your query. Please try again or rephrase your question. Error details: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01j3h4mrfbehht340pn5ra6xwh` on tokens per minute (TPM): Limit 6000, Used 0, Requested 6242. Please try again in 2.42s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
            "\n",
            "====================================================================================================\n",
            "\n",
            "\n",
            "--- Legal AI Knowledge Base ---\n",
            "\n",
            "Enter your query (or 'quit' to exit): quit\n"
          ]
        }
      ]
    }
  ]
}
